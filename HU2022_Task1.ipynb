{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mindspore"
      ],
      "metadata": {
        "id": "EidNBz_-gWnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ad891d-8e2a-4aaa-a251-d221b7f77125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mindspore in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (21.3)\n",
            "Requirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.6.1 in /usr/local/lib/python3.7/dist-packages (from mindspore) (5.9.0)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from mindspore) (1.7.3)\n",
            "Requirement already satisfied: asttokens>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (2.0.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.0->mindspore) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->mindspore) (3.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
        "from mindspore.train.callback import LossMonitor, SummaryCollector\n",
        "from mindspore import load_checkpoint, load_param_into_net\n",
        "import mindspore.dataset.transforms.c_transforms as C\n",
        "from mindspore.ops import ImageSummary, TensorSummary\n",
        "import mindspore.dataset.vision.c_transforms as CV\n",
        "from mindspore.common.initializer import Normal\n",
        "from mindspore.dataset.vision import Inter\n",
        "from mindspore.profiler import Profiler\n",
        "from mindspore import dtype as mstype\n",
        "from mindspore.nn import Accuracy\n",
        "import mindspore.dataset as ds\n",
        "from mindspore import context\n",
        "from mindspore import Tensor\n",
        "from mindspore import Model\n",
        "import mindspore.nn as nn\n",
        "import numpy as np\n",
        "import requests\n",
        "import argparse\n",
        "import shutil\n",
        "import os"
      ],
      "metadata": {
        "id": "S9s8j8WqqMmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXHb1BiQgNKN"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
        "parser.add_argument('--device_target', type=str, default=\"CPU\", choices=['Ascend', 'GPU', 'CPU'])\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(dataset_url, path):\n",
        "    filename = dataset_url.split(\"/\")[-1]\n",
        "    save_path = os.path.join(path, filename)\n",
        "    if os.path.exists(save_path):\n",
        "        return\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    res = requests.get(dataset_url, stream=True, verify=False)\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        for chunk in res.iter_content(chunk_size=512):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "train_path = \"datasets/MNIST_Data/train\"\n",
        "test_path = \"datasets/MNIST_Data/test\"\n",
        "\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte\", train_path)\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte\", train_path)\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte\", test_path)\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte\", test_path)"
      ],
      "metadata": {
        "id": "W1ZhRLlEgd1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
        "                   num_parallel_workers=1):\n",
        "    # Define the dataset.\n",
        "    mnist_ds = ds.MnistDataset(data_path)\n",
        "    resize_height, resize_width = 32, 32\n",
        "    rescale = 1.0 / 255.0\n",
        "    shift = 0.0\n",
        "    rescale_nml = 1 / 0.3081\n",
        "    shift_nml = -1 * 0.1307 / 0.3081\n",
        "\n",
        "    # Define the mapping to be operated.\n",
        "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
        "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)\n",
        "    rescale_op = CV.Rescale(rescale, shift)\n",
        "    hwc2chw_op = CV.HWC2CHW()\n",
        "    type_cast_op = C.TypeCast(mstype.int32)\n",
        "\n",
        "    # Use the map function to apply data operations to the dataset.\n",
        "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
        "    mnist_ds = mnist_ds.map(operations=[resize_op, rescale_op, rescale_nml_op, hwc2chw_op], input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
        "\n",
        "\n",
        "    # Perform shuffle, batch and repeat operations.\n",
        "    buffer_size = 10000\n",
        "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
        "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
        "    mnist_ds = mnist_ds.repeat(count=repeat_size)\n",
        "\n",
        "    return mnist_ds"
      ],
      "metadata": {
        "id": "HGqXIRrQgmwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Cell):\n",
        "    \"\"\"\n",
        "    Lenet network structure\n",
        "    \"\"\"\n",
        "    def __init__(self, num_class=10, num_channel=1):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Define the required operation.\n",
        "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
        "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
        "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
        "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.image_summary = ImageSummary()\n",
        "        self.tensor_summary = TensorSummary()\n",
        "\n",
        "    def construct(self, x):\n",
        "        self.image_summary(\"image\", x)\n",
        "        # Use the defined operation to construct a forward network.\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        self.tensor_summary(\"tensor\", x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the network.\n",
        "net = LeNet5()"
      ],
      "metadata": {
        "id": "NG8SBaosgq5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function.\n",
        "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')"
      ],
      "metadata": {
        "id": "yli2izdtgtpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer.\n",
        "net_opt = nn.Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "cO6qgUpmgv1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model saving parameters.\n",
        "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
        "# Use model saving parameters.\n",
        "ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)"
      ],
      "metadata": {
        "id": "mUyguvyogx9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_net(model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
        "    \"\"\"Define a training method.\"\"\"\n",
        "    # Load the training dataset.\n",
        "    ds_train = create_dataset(os.path.join(data_path, \"train\"), 32, repeat_size)\n",
        "    s_c = SummaryCollector(summary_dir = './summary_dir', collect_freq = 1)\n",
        "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(125), s_c], dataset_sink_mode=sink_mode)"
      ],
      "metadata": {
        "id": "kHooMYI_g2Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_net(model, data_path):\n",
        "    \"\"\"Define a validation method.\"\"\"\n",
        "    ds_eval = create_dataset(os.path.join(data_path, \"test\"))\n",
        "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
        "    print(\"{}\".format(acc))"
      ],
      "metadata": {
        "id": "3Od9TElpg4T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profiler = Profiler(output_path = './summary_dir/profiler_data')\n",
        "train_epoch = 1\n",
        "dataset_size = 1\n",
        "mnist_path = \"./datasets/MNIST_Data\"\n",
        "model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
        "train_net(model, train_epoch, mnist_path, dataset_size, ckpoint, False)\n",
        "profiler.analyse()\n",
        "test_net(model, mnist_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDyoJAzSg6CD",
        "outputId": "d196dddd-b6d9-4b65-fb7e-c09a8ef2b1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING] ME(1214:140712288098176,MainProcess):2022-05-04-10:44:02.405.106 [mindspore/profiler/profiling.py:1088] For 'Profiler', fail to get RANK_ID from environment, use 0 instead.\n",
            "[WARNING] ME(1214:140712288098176,MainProcess):2022-05-04-10:44:02.412.410 [mindspore/profiler/profiling.py:1120] The target dir already exists. There may be some old profiling data, and they will be rewritten in the end.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 step: 125, loss is 2.2993955612182617\n",
            "epoch: 1 step: 250, loss is 2.324800491333008\n",
            "epoch: 1 step: 375, loss is 2.3025295734405518\n",
            "epoch: 1 step: 500, loss is 2.309849500656128\n",
            "epoch: 1 step: 625, loss is 2.29313325881958\n",
            "epoch: 1 step: 750, loss is 2.2751083374023438\n",
            "epoch: 1 step: 875, loss is 2.2592031955718994\n",
            "epoch: 1 step: 1000, loss is 0.5033441185951233\n",
            "epoch: 1 step: 1125, loss is 0.15449242293834686\n",
            "epoch: 1 step: 1250, loss is 0.23691698908805847\n",
            "epoch: 1 step: 1375, loss is 0.2972780764102936\n",
            "epoch: 1 step: 1500, loss is 0.18391786515712738\n",
            "epoch: 1 step: 1625, loss is 0.13618116080760956\n",
            "epoch: 1 step: 1750, loss is 0.11137059330940247\n",
            "epoch: 1 step: 1875, loss is 0.058554165065288544\n",
            "{'Accuracy': 0.9649439102564102}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model for testing.\n",
        "param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
        "# Load parameters to the network.\n",
        "load_param_into_net(net, param_dict)"
      ],
      "metadata": {
        "id": "h-YB88I6hFV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875ce00e-2f9b-4a6b-c6b0-b07f0f4980ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a test dataset. If batch_size is set to 1, an image is obtained.\n",
        "ds_test = create_dataset(os.path.join(mnist_path, \"test\"), batch_size=1).create_dict_iterator()\n",
        "data = next(ds_test)\n",
        "\n",
        "# `images` indicates the test image, and `labels` indicates the actual classification of the test image.\n",
        "images = data[\"image\"].asnumpy()\n",
        "labels = data[\"label\"].asnumpy()\n",
        "\n",
        "# Use the model.predict function to predict the classification of the image.\n",
        "output = model.predict(Tensor(data['image']))\n",
        "predicted = np.argmax(output.asnumpy(), axis=1)\n",
        "\n",
        "# Output the predicted classification and the actual classification.\n",
        "print(f'Predicted: \"{predicted[0]}\", Actual: \"{labels[0]}\"')"
      ],
      "metadata": {
        "id": "bZbcl0KphSUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29227240-07e2-4fc5-85e7-d93f12c71dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"1\", Actual: \"1\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_name = 'summary_dir'\n",
        "directory_name = '/content/summary_dir'\n",
        "shutil.make_archive(zip_name, 'zip', directory_name)"
      ],
      "metadata": {
        "id": "5FfSz8AvjM5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9b31404-520d-4c00-d40b-d8cab7432c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/summary_dir.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}